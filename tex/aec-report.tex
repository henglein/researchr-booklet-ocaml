\header{AEC Chairs' Report}{}{}{Artifact Evaluation Chairs' Report}
\label{aec}

% In the programming languages community,

Artifact evaluation attempts to evaluate the scientific claims of a
research paper against a set of supporting byproducts or
``artifacts''. Many documents can qualify as artifacts, including a
program’s source code, a dataset, a test suite, a mechanized proof, or
a model. The primary goal of the artifact evaluation process is to
encourage authors to create artifacts that can be shared and used by
others as a basis for new activities. This process both encourages
authors to be precise in their claims and provides public recognition
for the effort needed to create reusable artifacts. To incentivize
these beneficial behaviors, since POPL 2015, authors of accepted
papers have been invited to submit artifacts for evaluation by an
Artifact Evaluation Committee (AEC). This report describes the
artifact evaluation process for POPL 2019, discussing what worked well
and what can still be improved.

Members of the POPL Program Committee were invited to nominate senior PhD
students, postdocs, researchers, or themselves to serve on the AEC.
% Participation in the AEC can provide researchers with useful insight into the
% value of artifacts and the process of artifact evaluation, and also helps
% establish community norms for artifacts.
Starting from the nominations and after some targeted search aimed at
balancing the AEC, the chairs formed a committee of 30 members. After
decisions were announced for POPL 2019, authors of conditionally
accepted papers were invited to submit artifacts for evaluation. By
design, the artifact evaluation process had no effect on which papers
were chosen to appear at POPL.  Authors had one week to respond to the
call for artifacts and submit their artifact for evaluation. The
committee then had a little more than three weeks to evaluate each
submission against the accompanying paper's claims.
% The submission guidelines asked that the authors create a single web
% page that contains the artifact and instructions for installing and
% using the artifact, while allowing for some exceptional cases.
Out of 77 papers accepted this year, 43 artifacts were submitted for
evaluation (56\%), which was a record number. For comparison, POPL
2018 had 40 artifacts submitted out of 66 conditionally accepted
(61\%), POPL 2017 had 31 artifacts submitted out of 64 accepted papers
(48\%), POPL 2016 had 25 artifacts submitted out of 59 accepted papers
(42\%), and POPL 2015 had 29 artifacts submitted out of 52 accepted
papers (56\%).

% This is the third year when the POPL AEC has decided to not accept paper proofs
% in the artifact evaluation process. The AEC lacks the time and often the
% expertise to carefully review paper proofs. We also hope that reserving the
% artifact evaluated badge to mechanized proofs that are easy to check and reuse
% will incentivize more of the POPL authors to mechanize their metatheory in a
% proof assistant. Paper proofs can still be uploaded to the ACM Digital Library
% together with the final version, and this policy seems to have caused no strong
% objections so far.

% The schedule for artifact evaluation was again tight this year: the
% AEC only had only two weeks to evaluate whether the artifacts met the
% expectations set by the paper.
%
The AEC members did an amazing job at evaluating artifacts and
providing helpful feedback.
%
The average AEC reviewer submitted 2.7 reviews and almost all reviews
 were submitted on time, a big achievement given the tight schedule.
%
After this, the AEC held an online discussion to decide, for each
artifact, if it met, exceeded, or fell below the expectations set by
its paper.
%
The scores were to a very large extent positive (the average merit
score was 3.99 out of 5).
%
In the end 42 out of 43 submitted artifacts were judged to meet or
exceed expectations (97.7\%).
%
We believe that the large percentage of positive outcomes is simply
confirming that the POPL community produces great artifacts.

Beyond the high quality of submitted artifacts, another reason for the
high acceptance rate is that we configured the HotCRP conference
management system so that the anonymous reviewers could ask questions
using author-visible comments and get immediate answers from the
authors. In addition, any finalized reviews were immediately made
available to the authors, and authors could also comment on those
reviews. Eddie Kohler added this feature to HotCRP specially for last
year's AEC, so that reviewers could directly and anonymously ask the
authors for help or clarifications or fixes using the HotCRP
interface. Feedback about this setup from reviewers and authors was
overwhelmingly positive, and we plan to keep doing this in the future
and we warmly recommend it to the AECs of other conferences.

As always, there are some aspects of the process which could be
improved. One recurring struggle is getting enough time for artifact
evaluation: both this and last year the committee only had two weeks
for the actual reviewing. This works in the limit, but adds quite a
bit of stress. Even getting these two weeks required hard work: the
publishing process was revamped for this edition of POPL, and we had
to ensure that the AEC feedback made it to the authors in time for
them to potentially incorporate this feedback into the final version of their
paper (e.g. if the paper sets certain inaccurate expectations about
the artifacts, which is not uncommon).

The ACM recently introduced a tiered badging system, allowing each
paper to be assigned “Artifacts Available”, “Artifacts Evaluated
Functional”, and “Artifacts Evaluated Reusable” badges. We chose to
adopt these new distinctions incrementally: last year we rolled out
the “Artifacts Available” badge, and this year we introduced the
“Functional” and “Reusable” badges.

For the past two years, we have adopted a policy of only awarding the
“Artifacts Available” badge to papers that have successfully passed
artifact evaluation and whose authors have uploaded the evaluated
artifact to the ACM DL. This is stricter than the ACM's default policy
of awarding this badge to any paper that uploads materials to the ACM
Digital Library. This policy has also been adopted by the AEC chairs
of the other PACMPL conferences, ensuring a high-standard for artifact
badges across the community. We had no complaints about this
restriction to the ACM policy, and this year 88\% of the papers with
successfully accepted artifacts got this extra badge. The only
remaining (minor) issue is improving the publisher's submission forms
so that uploads of AEC-accepted artifacts are clearly separated from
uploads of unevaluated materials, in order to reduce unnecessary
guesswork when awarding this badge.

The distinction between “Functional” and “Reusable” artifacts that we
implemented this year was trickier, since the official ACM guidelines
were very open-ended. We came up with a simple policy that tried to
incentivize authors to: (1) make their artifacts as good as possible
and (2) release their artifacts under an open source license. We felt
that point (1) was useful, since with the AEC's historical acceptance
rate of almost 100\% might not incentivize authors to do the extra
work required to make their artifact great. Point (2) was useful since
having the source code available under an open source license makes
easy reuse by others possible (at least in principle), while the
“Artifacts Available” badge is also awarded to artifacts that are only
released as a binary blob and/or under onerous licenses, as long as
they are permanently archived. Finally, a pleasant potential side
effect of having two levels of badges was that it could also encourage
more discussions in order to sort the artifacts. Last year, the very
high average scores limited discussions to the few artifacts with the
lowest scores.

After some quick iterations, we ended up with the following % text for our
policy: “The AEC will offer the ‘Artifacts Evaluated -
Reusable’ badge to the artifacts that receive above average scores and
that are additionally made available in a way that enables reuse (for
software artifacts this means releasing the artifact under an open
source license, ideally on a platform like GitHub, GitLab, BitBucket,
SourceForge, etc). All other artifacts that meet expectations will get
the default ‘Artifacts Evaluated - Functional’ badge.” From the past
years' data, we were able to predict that the average would be around
4.0 (which corresponds to “Accept” on the usual 5-point scale in
HotCRP), and we communicated to the reviewers from the beginning that
artifacts with a score of 4.0 or higher will be good candidates for
the Reusable badge. % Our average score this year turned out 3.99.
%
Most artifacts with an average score of 4.0 or higher that were made
open source were given the Reusable badge. In the end 69\% of the
accepted artifacts were awarded the Reusable badge and 31\% received
the Functional badge.

% Our impression is that
This simple policy seemed to achieve its goals: 93\% of the submitted
artifacts were released as open source. In addition, many authors put
serious energy into answering questions and quickly fixing, improving,
or documenting their artifacts in response to reviewer feedback, even
when it was clear that their artifact would be accepted anyway, so the
“Reusable” badge seems to incentivize authors to produce better
artifacts. Finally, in order to decide between the two badges all the
artifacts were discussed this year, which was great progress from last
year’s limited discussions. Beyond the increase in discussions, making
this new distinction using the simple policy above did not make the
work of reviewers more difficult.


In summary, we managed to make good use of the changes of the badging
system introduced by the ACM to further incentivize artifact authors
to produce better and more reusable artifacts. There are undoubtedly
philosophical discussions about whether there are better ways to
interpret the “Available”, “Functional”, and “Reusable” designations
of the ACM. We feel that the simple policies we have devised for
POPL have the advantage of being easy to understand, easy to enforce,
and easy to integrate into our already well-working artifact
evaluation process. We appreciated introducing these changes
incrementally on our own, and we believe this positive experience can
inform potential future standardization efforts at the SIGPLAN level.

We thank the authors of all the submitted artifacts for their work in
preparing and documenting their research output. We hope that they
found the feedback from the AEC to be helpful. And last but not least,
we are immensely grateful to the members of the AEC for their energy
and enthusiasm.

\begin{flushright}
\textit{Benjamin Delaware, Purdue University} \\
\textit{C\u{a}t\u{a}lin Hri\c{t}cu, Inria Paris} \\
POPL 2019 Artifact Evaluation Chairs
% \medskip \\
\end{flushright}

\newpage
